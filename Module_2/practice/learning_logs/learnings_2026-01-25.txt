================================================================================
                    DATA ENGINEERING ZOOMCAMP - LEARNING LOG
                              Date: January 25-27, 2026
================================================================================

SESSION DURATION: Days 8-10 (Module 2: Workflow Orchestration)

================================================================================
                     TOPIC 1: KESTRA - WORKFLOW ORCHESTRATION
================================================================================

Kestra is an open-source workflow orchestration platform for building
data pipelines, ETL processes, and automation workflows.

--------------------------------------------------------------------------------
DOCKER COMPOSE SETUP
--------------------------------------------------------------------------------

services:
  # NYC Taxi PostgreSQL database
  pgdatabase:
    image: postgres:18
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: root
      POSTGRES_DB: ny_taxi
    ports:
      - "5432:5432"
    volumes:
      - ny_taxi_postgres_data:/var/lib/postgresql

  # pgAdmin for database management
  pgadmin:
    image: dpage/pgadmin4
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@admin.com
      - PGADMIN_DEFAULT_PASSWORD=root
    ports:
      - "8085:80"

  # Kestra's backend PostgreSQL
  kestra_postgres:
    image: postgres:18
    environment:
      POSTGRES_DB: kestra
      POSTGRES_USER: kestra
      POSTGRES_PASSWORD: k3str4
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 30s
      timeout: 10s
      retries: 10

  # Kestra orchestrator
  kestra:
    image: kestra/kestra:v1.1
    user: "root"
    command: server standalone
    ports:
      - "8080:8080"
      - "8081:8081"
    depends_on:
      kestra_postgres:
        condition: service_started

--------------------------------------------------------------------------------
KESTRA CREDENTIALS
--------------------------------------------------------------------------------

| Service     | URL                    | Username          | Password    |
|-------------|------------------------|-------------------|-------------|
| Kestra UI   | http://localhost:8080  | admin@kestra.io   | Admin1234   |
| pgAdmin     | http://localhost:8085  | admin@admin.com   | root        |
| PostgreSQL  | localhost:5432         | root              | root        |


================================================================================
                     TOPIC 2: KESTRA FLOWS COMPLETED (01-05)
================================================================================

--------------------------------------------------------------------------------
FLOW 01: Hello World
--------------------------------------------------------------------------------
- Basic introduction to Kestra flow structure
- Understanding YAML syntax for workflows
- Simple log output task

--------------------------------------------------------------------------------
FLOW 02: Python Scripts
--------------------------------------------------------------------------------
- Running Python code in Kestra
- Using containerImage: python:3.11-alpine
- Input/output file handling between tasks
- Environment variables in scripts

--------------------------------------------------------------------------------
FLOW 03: Getting Started Data Pipeline
--------------------------------------------------------------------------------
Pipeline: Extract → Transform → Query

id: 03_getting_started_data_pipeline
namespace: zoomcamp

tasks:
  - id: extract
    type: io.kestra.plugin.core.http.Download
    uri: https://dummyjson.com/products

  - id: transform
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-alpine
    inputFiles:
      data.json: "{{outputs.extract.uri}}"
    outputFiles:
      - "*.json"
    script: |
      # Filter products to keep only brand and price

  - id: query
    type: io.kestra.plugin.jdbc.duckdb.Queries
    sql: |
      SELECT brand, round(avg(price), 2) as avg_price
      FROM read_json_auto('products.json')
      GROUP BY brand ORDER BY avg_price DESC;

--------------------------------------------------------------------------------
FLOW 04: Postgres Taxi (Local DB)
--------------------------------------------------------------------------------
Pipeline: Extract CSV → Create Tables → Load → Merge

Key Features:
- Inputs: taxi type (yellow/green), year, month
- Downloads compressed CSV from GitHub releases
- Creates staging and final tables
- Uses MD5 hash for unique_row_id (deduplication)
- MERGE statement for upsert operations

Variables:
  file: "{{inputs.taxi}}_tripdata_{{inputs.year}}-{{inputs.month}}.csv"
  staging_table: "public.{{inputs.taxi}}_tripdata_staging"
  table: "public.{{inputs.taxi}}_tripdata"

Tasks:
  1. set_label - Add execution labels
  2. extract - wget CSV.gz and gunzip
  3. if_yellow_taxi / if_green_taxi - Conditional branching
  4. create_table - CREATE TABLE IF NOT EXISTS
  5. create_staging_table - Temporary loading table
  6. truncate_staging_table - Clear staging
  7. copy_in_to_staging_table - Bulk CSV load
  8. add_unique_id_and_filename - MD5 hash + metadata
  9. merge_data - MERGE INTO final table
  10. purge_files - Cleanup execution files

Plugin Defaults (connection reuse):
  pluginDefaults:
    - type: io.kestra.plugin.jdbc.postgresql
      values:
        url: jdbc:postgresql://pgdatabase:5432/ny_taxi
        username: root
        password: root

--------------------------------------------------------------------------------
FLOW 05: Postgres Taxi Scheduled
--------------------------------------------------------------------------------
Same as Flow 04 but with scheduling and backfills.

Trigger Configuration:
  triggers:
    - id: schedule
      type: io.kestra.plugin.core.trigger.Schedule
      cron: "0 9 * * *"  # Daily at 9 AM UTC

Backfill Feature:
- Run historical data by specifying date range
- Useful for catching up on missed data
- Green taxi 2019 data used for demo


================================================================================
                     TOPIC 3: KESTRA KEY CONCEPTS
================================================================================

| Concept      | Description                                          |
|--------------|------------------------------------------------------|
| Flows        | YAML-defined workflows with id and namespace         |
| Tasks        | Individual steps (HTTP, SQL, Python, Shell, etc.)    |
| Inputs       | User-provided values (SELECT, STRING, ARRAY, etc.)   |
| Outputs      | Task results accessible via {{outputs.taskId.xxx}}   |
| Variables    | Reusable values with {{render(vars.name)}}           |
| Triggers     | Schedule (cron), Webhook, Flow completion            |
| Namespaces   | Logical grouping (e.g., zoomcamp)                    |
| Labels       | Metadata for filtering executions                    |
| Conditions   | If/Else branching based on inputs                    |
| pluginDefaults | Shared config across tasks (e.g., DB connection)  |


================================================================================
                     TOPIC 4: DOCKER VOLUMES
================================================================================

Docker volumes persist data outside of container lifecycle.

| Command                          | Purpose                              |
|----------------------------------|--------------------------------------|
| docker volume ls                 | List all volumes                     |
| docker volume inspect NAME       | Show volume details and mount path   |
| docker volume rm NAME            | Remove a specific volume             |
| docker volume prune              | Remove all unused volumes            |

# Browse volume contents using alpine
docker run -it --rm -v VOLUME_NAME:/data alpine ls -la /data


================================================================================
                              KEY TAKEAWAYS
================================================================================

1. Kestra uses YAML for declarative workflow definitions
2. Tasks can be Python scripts, SQL queries, HTTP calls, Shell commands
3. {{outputs.taskId.field}} syntax passes data between tasks
4. pluginDefaults reduces repetition for shared configurations
5. Conditional flows with io.kestra.plugin.core.flow.If
6. MERGE statement enables idempotent data loading (upserts)
7. Scheduling with cron expressions and backfill for historical data
8. MD5 hashing creates deterministic unique IDs for deduplication


================================================================================
                            FILES & FLOWS
================================================================================

Module_2/
└── practice/
    ├── docker-compose.yaml
    └── learning_logs/
        └── learnings_2026-01-25.txt

Kestra Flows Completed:
├── 01_hello_world
├── 02_python
├── 03_getting_started_data_pipeline
├── 04_postgres_taxi
└── 05_postgres_taxi_scheduled


================================================================================
                               END OF LOG
================================================================================
